{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Speech-to-text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uigiporc/icon-sr/blob/main/Speech_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdS873K-htUg"
      },
      "source": [
        "# Initializing the environement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphDFbBHkyr-"
      },
      "source": [
        "##Check VM region\n",
        "\n",
        "## **`If the region is not us-central1, don't proceed with the model `**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq2FfqJodpOw",
        "outputId": "67b563d7-1084-4c93-c5ca-ad0823e4fb8b"
      },
      "source": [
        "import requests\n",
        "import ipaddress\n",
        "import time\n",
        "\n",
        "def check_region(ip):\n",
        "    response = requests.get(\"https://www.gstatic.com/ipranges/cloud.json\")\n",
        "    data = response.json()\n",
        "    try:\n",
        "      for prefix in data['prefixes']:\n",
        "        if 'ipv4Prefix' in prefix:\n",
        "          if ipaddress.IPv4Address(ip) in ipaddress.IPv4Network(prefix['ipv4Prefix']):\n",
        "            print(f\"Found network address for {ip}: {prefix['ipv4Prefix']} Region: {prefix['scope']}\")\n",
        "\n",
        "            #Whenever we don't get a TPU in US-CENTRAL1, we kill the process for data transfer costs reasons, since the bucket is located in that region\n",
        "            if 'us-central1' not in prefix['scope']:\n",
        "              raise ValueError(f\"Region found: {prefix['scope']}, Desidered region: us-central1\")\n",
        "            return\n",
        "      raise ValueError(f\"Can't find network for this machine with ip {ip}\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(\"Killing the process in 5 seconds.\")\n",
        "      time.sleep(5)\n",
        "      !kill -9 -1\n",
        "\n",
        "ip = requests.get('https://ipecho.net/plain').text\n",
        "check_region(ip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found network address for 35.188.133.199: 35.188.128.0/18 Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZPlnQWImr0m"
      },
      "source": [
        "## Resolve dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojF0lz6moRT"
      },
      "source": [
        "!pip install jiwer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from jiwer import wer\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FLZjTVlqCi_"
      },
      "source": [
        "# Equals to BATCH_SIZE if training on CPU/GPU, BATCH_SIZE*8 when training on TPU\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Set to True if using a Google Cloud Bucket as your dataset source. Mandatory when training on TPU.\n",
        "USE_GCS = True\n",
        "\n",
        "# Base bucket path\n",
        "GCS_BUCKET_PATH = \"gs://progetto-icon-cucinotta-porcelli\"\n",
        "\n",
        "# Google Cloud project name used to sync permissions\n",
        "GCS_PROJECT_NAME = 'progetto-icon'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPbpvwuIlf3I"
      },
      "source": [
        "## Initialize backend (TPU/GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNoZyUcrnPHx"
      },
      "source": [
        "TPU_FLAG = False\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  assert USE_GCS\n",
        "  TPU_FLAG = True\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  print('No TPU found. Falling back to GPU')\n",
        "  pass\n",
        "except AssertionError:\n",
        "  raise BaseException('A Google Cloud Storage bucket must be specified when using a TPU. Local files are not supported.')\n",
        "\n",
        "if TPU_FLAG:\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "  gpus = tf.config.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      # Currently, memory growth needs to be the same across GPUs\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "      # Memory growth must be set before GPUs have been initialized\n",
        "      print(e)\n",
        "  else:\n",
        "    print('No GPU found. Falling back to CPU (expect very long training times)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTwmBaulr8G"
      },
      "source": [
        "### Authenticate to Google Cloud and set a project\n",
        "### Download dataset from drive if cannot authenticate to GCS (Cloud Storage Bucket)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vqJiUUSuTK7",
        "outputId": "d4a0265c-58bb-4aaf-ad9a-2693b55414cf"
      },
      "source": [
        "from google.colab import drive, auth\n",
        "\n",
        "print(\"Authenticating Colab session:\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "if USE_GCS:\n",
        "  print(\"Authenticating to Google Cloud to use GCS:\")\n",
        "  #https://stackoverflow.com/questions/61448884/how-to-connect-to-private-storage-bucket-using-the-google-colab-tpu\n",
        "  !gcloud config set project {GCS_PROJECT_NAME}\n",
        "  project_name = !gcloud config get-value project\n",
        "  print(f\"Connected to {project_name} project\")\n",
        "  try: \n",
        "    tf.io.gfile.exists(GCS_BUCKET_PATH)\n",
        "  except:\n",
        "    raise ValueError('Bucket name does not exist')\n",
        "else:\n",
        "  print(\"Connecting to Google Drive:\")\n",
        "  drive.mount('/content/drive')\n",
        "  # make space and create a local folder for the dataset for better I/O performances\n",
        "  !rm -r sample_data/\n",
        "  !mdkir dataset\n",
        "  !mkdir checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticating Colab session:\n",
            "Authenticating to Google Cloud to use GCS:\n",
            "Updated property [core/project].\n",
            "Connected to ['progetto-icon'] project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IomfItBzi5E9"
      },
      "source": [
        "# Processing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o751TF4vnRi7"
      },
      "source": [
        "## Dataset Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUDI_jBLnVPu"
      },
      "source": [
        "def parse_tfr_audio_element(element):\n",
        "    feats = {\n",
        "        'spectrogram': tf.io.FixedLenFeature([], tf.string),\n",
        "        'text': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, feats)\n",
        "\n",
        "    spectrogram = tf.io.parse_tensor(content['spectrogram'], tf.float32) # note that we change the data type to float32\n",
        "    text = tf.io.parse_tensor(content['text'], tf.int64)\n",
        "\n",
        "    return spectrogram, text\n",
        "\n",
        "def load_dataset(train_files, batch_size=BATCH_SIZE, buf_size=5000):\n",
        "  # disable order, increase speed\n",
        "  ignore_order = tf.data.Options()\n",
        "  ignore_order.experimental_deterministic = False\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset(filenames=train_files).with_options(ignore_order)\n",
        "  dataset = dataset.map(parse_tfr_audio_element, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  # don't cache large datasets that don't fit in memory\n",
        "  #ds_train = ds_train.cache()\n",
        "\n",
        "  # shuffle the entire dataset to get good randomization \n",
        "  # we get true randomization if buffer_size >= # of examples in dataset\n",
        "  # when dealing with big/huge datasets, loading the full dataset into ram and shuffling might not be possible\n",
        "  dataset = dataset.shuffle(buffer_size=buf_size)\n",
        "\n",
        "  # pad to the closest multiple of 128 that fits our data for better TPU performances\n",
        "  # pad spectrogram to 3328 timesteps and label to 640 characters\n",
        "  dataset = dataset.padded_batch(batch_size, padded_shapes=([3328,128], [640]), drop_remainder=True)\n",
        "\n",
        "  #improve pipeline performance\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9xVy9FfndGD"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSaO8Rl5ng0a",
        "outputId": "4fab931b-3f4d-4bf1-862f-10b5b5e6b412"
      },
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ' \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)\n",
        "\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "if USE_GCS:\n",
        "  dataset_filenames = tf.io.gfile.glob(GCS_BUCKET_PATH + \"/librispeech-train-360-clean/librispeech-train-clean-360.tfrecord*\")\n",
        "  test_files = tf.io.gfile.glob(GCS_BUCKET_PATH + \"/test-clean/librispeech-test-clean.tfrecord*\")\n",
        "else:\n",
        "  drive_filenames = tf.io.gfile.glob(\"/content/drive/MyDrive/Datasets/Librispeech/dev-clean/*.tfrecord\")\n",
        "  !cp {drive_filenames} /content/dataset\n",
        "  dataset_filenames = tf.io.gfile.glob(\"/content/dataset/*.tfrecord\")\n",
        "\n",
        "# 80/20 split for train/eval dataset\n",
        "split_ind = int(0.8 * len(dataset_filenames))\n",
        "training_files, eval_files = dataset_filenames[:split_ind], dataset_filenames[split_ind:]\n",
        "\n",
        "print(\"Train TFRecord Files:\", len(training_files))\n",
        "print(\"Validation TFRecord Files:\", len(eval_files))\n",
        "print(\"Test TFRecord Files\", len(test_files))\n",
        "#training_files = [train_filenames]              \n",
        "#eval_files = [validation_filenames]\n",
        "\n",
        "if TPU_FLAG:\n",
        "  print(f\"Detected TPU with {tpu_strategy.num_replicas_in_sync} cores, increasing batch size for efficiency\")\n",
        "  batch_size = BATCH_SIZE * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "ds_train = load_dataset(training_files, batch_size)\n",
        "ds_eval = load_dataset(eval_files, batch_size)\n",
        "ds_test = load_dataset(test_files, batch_size)\n",
        "\n",
        "print(f\"Effective batch size: {batch_size}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TFRecord Files: 80\n",
            "Validation TFRecord Files: 20\n",
            "Test TFRecord Files 10\n",
            "Detected TPU with 8 cores, increasing batch size for efficiency\n",
            "Effective batch size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AX6Lzqgpq50"
      },
      "source": [
        "### Test if the dataset has been processed and loaded correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isGlu5eb18mx"
      },
      "source": [
        "#for batch in ds_train.take(1):\n",
        "#    print(batch)\n",
        "#    spectrogram = batch[0][0].numpy()\n",
        "#    print(batch[0][0])\n",
        "#    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "#    label = batch[1][0]\n",
        "#    # Spectrogram\n",
        "#    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSEkiOfHjcxF"
      },
      "source": [
        "##Custom subclass of ModelCheckpoint \n",
        "##At the end of each epoch, calculates and prints the Word Error Rate, then saves the model as a tf format checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guJvhYn9jhOm"
      },
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "class WERCheckpoint(keras.callbacks.ModelCheckpoint):\n",
        "  \n",
        "  def __init__(self, filepath, dataset, verbose=0, save_best_only=False,\n",
        "    save_weights_only=False, mode='auto', save_freq='epoch',\n",
        "    options=None):\n",
        "    super().__init__(filepath = filepath,\n",
        "                         verbose = verbose,\n",
        "                         save_best_only = save_best_only,\n",
        "                         save_weights_only = save_weights_only,\n",
        "                         mode = mode,\n",
        "                         save_freq = save_freq,\n",
        "                         options = options)\n",
        "    self.dataset = dataset\n",
        "\n",
        "  def _print_wer(self, wer, predictions, targets):\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"Word Error Rate: {wer:.4f}\")\n",
        "    print(\"-\" * 100)\n",
        "    for i in np.random.randint(0, len(predictions), 2):\n",
        "      print(f\"Target    : {targets[i]}\")\n",
        "      print(f\"Prediction: {predictions[i]}\")\n",
        "      print(\"-\" * 100)\n",
        "\n",
        "  def _save_model(self, epoch, batch, logs):\n",
        "    \"\"\"Saves the model.\n",
        "\n",
        "    Args:\n",
        "        epoch: the epoch this iteration is in.\n",
        "        batch: the batch this iteration is in. `None` if the `save_freq`\n",
        "          is set to `epoch`.\n",
        "        logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n",
        "    \"\"\"\n",
        "    logs = logs or {}\n",
        "\n",
        "    if isinstance(self.save_freq,\n",
        "                  int) or self.epochs_since_last_save >= self.period:\n",
        "      # Block only when saving interval is reached.\n",
        "      self.epochs_since_last_save = 0\n",
        "      filepath = self._get_file_path(epoch, batch, logs)\n",
        "\n",
        "      try:\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset.take(5):\n",
        "          X, y = batch\n",
        "          pred = model.predict(X)\n",
        "          batch_predictions = decode_batch_predictions(pred) \n",
        "          predictions.extend(batch_predictions)\n",
        "          for label in y:\n",
        "            label = (\n",
        "                tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "            targets.append(label)\n",
        "          \n",
        "        if self.save_best_only:\n",
        "          current = wer(targets, predictions)\n",
        "          if self.monitor_op(current, self.best):\n",
        "            if self.verbose > 0:\n",
        "              print('\\nEpoch %05d: WER improved from %0.5f to %0.5f,'\n",
        "              ' saving model to %s' % (epoch + 1,\n",
        "                                       self.best, current, filepath))\n",
        "              self._print_wer(current, predictions, targets)\n",
        "              self.best = current\n",
        "              if self.save_weights_only:\n",
        "                self.model.save_weights(\n",
        "                    filepath, overwrite=True, options=self._options)\n",
        "              else:\n",
        "                self.model.save(filepath, overwrite=True, options=self._options)\n",
        "            else:\n",
        "              if self.verbose > 0:\n",
        "                print('\\nEpoch %05d: WER did not improve from %0.5f' %\n",
        "                      (epoch + 1, self.best))\n",
        "                self._print_wer(current, predictions, targets)\n",
        "        else:\n",
        "          if self.verbose > 0:\n",
        "            print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "            current = wer(targets, predictions)\n",
        "            self._print_wer(current, predictions, targets)\n",
        "          if self.save_weights_only:\n",
        "            self.model.save_weights(\n",
        "                filepath, overwrite=True, options=self._options)\n",
        "          else:\n",
        "            self.model.save(filepath, overwrite=True, options=self._options)\n",
        "\n",
        "        self._maybe_remove_file()\n",
        "      except IsADirectoryError as e:  # h5py 3.x\n",
        "        raise IOError('Please specify a non-directory filepath for '\n",
        "                      'ModelCheckpoint. Filepath used is an existing '\n",
        "                      'directory: {}'.format(filepath))\n",
        "      except IOError as e:  # h5py 2.x\n",
        "        # `e.errno` appears to be `None` so checking the content of `e.args[0]`.\n",
        "        if 'is a directory' in str(e.args[0]).lower():\n",
        "          raise IOError('Please specify a non-directory filepath for '\n",
        "                        'ModelCheckpoint. Filepath used is an existing '\n",
        "                        'directory: {}'.format(filepath))\n",
        "        # Re-throw the error for any other causes.\n",
        "        raise e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxVoK5P2h_6H"
      },
      "source": [
        "## Defining the Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcnWmN1Pn-zt"
      },
      "source": [
        "if USE_GCS:\n",
        "  checkpoint_filepath = GCS_BUCKET_PATH + '/360h_checkpoints/saved_cloud_model.{epoch:03d}'\n",
        "else:\n",
        "  checkpoint_filepath = '/content/drive/MyDrive/Datasets/Librispeech/dev-clean/checkpoint/saved_model.{epoch:03d}'\n",
        "\n",
        "custom_cb = WERCheckpoint(filepath=checkpoint_filepath,\n",
        "                          dataset=ds_eval,\n",
        "                          verbose = 1,\n",
        "                          save_weights_only=False,\n",
        "                          save_best_only=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S5X0p6qkbfW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJraSGT6oPs-"
      },
      "source": [
        "### Define custom loss function based on Connectionist Temporal Classification (CTC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ltTFEM7GLx"
      },
      "source": [
        "def CTCLossNew(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])\n",
        "    label_length = tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1])\n",
        "\n",
        "    # the input must not be softmaxed\n",
        "    y_pred = tf.math.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-7)\n",
        "\n",
        "    loss = tf.nn.ctc_loss(\n",
        "            labels=y_true,\n",
        "            logits=y_pred,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            logits_time_major=True,\n",
        "            blank_index=-1)\n",
        "    \n",
        "    return tf.expand_dims(loss,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26EAmEbkke0c"
      },
      "source": [
        "###Define model and how to load previous checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCN5KaDEuiml"
      },
      "source": [
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((3328, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLossNew)\n",
        "    return model\n",
        "\n",
        "# This function loads the model (including the state of the optimizer) and weights\n",
        "def get_model_from_cp(path):\n",
        "  if tf.io.gfile.listdir(path):\n",
        "    print(f\"Checkpoint found. Loading...\")\n",
        "    model = keras.models.load_model(os.path.join(path,tf.io.gfile.listdir(path)[-1]), custom_objects={ 'CTCLossNew': CTCLossNew })\n",
        "    #print(tf.io.gfile.listdir(path)[-1])\n",
        "    epochs = int(tf.io.gfile.listdir(path)[-1][-4:-1])\n",
        "    print(f\"Restored model from checkpoint {epochs}\")\n",
        "  else:\n",
        "    print(f\"Could not find checkpoint in {path}. Building new model:\")\n",
        "    model = build_model(input_dim=128, output_dim=char_to_num.vocabulary_size(), rnn_units=256)\n",
        "    epochs = 0\n",
        "\n",
        "  return model, epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqGPvrwZWJqQ",
        "outputId": "1ec3f5da-01f2-400f-c11e-a097fd33d26f"
      },
      "source": [
        "if TPU_FLAG:\n",
        "  # we need to instantiate and compile the model, optimizer and metrics in the TPU scope\n",
        "  with tpu_strategy.scope():\n",
        "    print(\"Compiling model for TPU\")\n",
        "    model, last_epoch = get_model_from_cp('gs://progetto-icon-cucinotta-porcelli/360h_checkpoints')\n",
        "else:\n",
        "  print(\"Compiling model for GPU/CPU\")\n",
        "  #model, last_epoch = get_model_from_cp('/content/drive/MyDrive/Datasets/Librispeech/dev-clean/checkpoint/')\n",
        "  model, last_epoch = get_model_from_cp('gs://progetto-icon-cucinotta-porcelli/360h_checkpoints')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model for TPU\n",
            "Checkpoint found. Loading...\n",
            "Restored model from checkpoint 78\n",
            "Model: \"DeepSpeech_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 3328, 128)]       0         \n",
            "_________________________________________________________________\n",
            "expand_dim (Reshape)         (None, 3328, 128, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv2D)              (None, 1664, 64, 32)      14432     \n",
            "_________________________________________________________________\n",
            "conv_1_bn (BatchNormalizatio (None, 1664, 64, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_1_relu (ReLU)           (None, 1664, 64, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 1664, 32, 32)      236544    \n",
            "_________________________________________________________________\n",
            "conv_2_bn (BatchNormalizatio (None, 1664, 32, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_2_relu (ReLU)           (None, 1664, 32, 32)      0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 1664, 1024)        0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1664, 512)         1969152   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 1664, 512)         1182720   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 1664, 512)         1182720   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 1664, 512)         1182720   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 1664, 512)         1182720   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1664, 512)         262656    \n",
            "_________________________________________________________________\n",
            "dense_1_relu (ReLU)          (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 1664, 512)         0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1664, 30)          15390     \n",
            "=================================================================\n",
            "Total params: 7,229,310\n",
            "Trainable params: 7,229,182\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHmipyc0orr6"
      },
      "source": [
        "# Optional monitoring with TensorBoard\n",
        "### Refer to **[THIS LINK](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/profiling_tpus_in_colab.ipynb#scrollTo=erj2-mPpvqbG)** for proper usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPJQe7F7Rm6o"
      },
      "source": [
        "#https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/profiling_tpus_in_colab.ipynb\n",
        "!pip install -U tensorboard-plugin-profile\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Get TPU profiling service address. This address will be needed for capturing\n",
        "# profile information with TensorBoard in the following steps.\n",
        "service_addr = tpu.get_master().replace(':8470', ':8466')\n",
        "print(service_addr)\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3qQSpWKkm5r"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wms01DKRsni"
      },
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 20 + last_epoch\n",
        "# Callback function to check transcription on the val set.\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    initial_epoch=last_epoch,\n",
        "    validation_data=ds_eval,\n",
        "    epochs=epochs,\n",
        "    callbacks=[custom_cb],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHAK0F_p1T1A"
      },
      "source": [
        "# Test model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flHy0rs2Bk8A"
      },
      "source": [
        "## Test WER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6qMSt_B1rzs"
      },
      "source": [
        "if TPU_FLAG:\n",
        "  with tpu_strategy.scope():\n",
        "    # We don't need to compile the model when doing inference\n",
        "    model = keras.models.load_model('gs://progetto-icon-cucinotta-porcelli/360h_checkpoints/saved_cloud_model.079', compile=False)\n",
        "    number_of_examples = 0\n",
        "    temp_wer = 0\n",
        "    i=0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    # test the whole dataset\n",
        "    for batch in ds_test:\n",
        "\n",
        "      print(f\"Processing batch #{i}\")\n",
        "      i = i+1\n",
        "\n",
        "      #don't fill the RAM with predictions\n",
        "      if len(targets) >= 512:\n",
        "        #calculate weighted average\n",
        "        temp_wer = temp_wer + wer(targets, predictions) * len(targets)\n",
        "        number_of_examples = number_of_examples + len(targets)\n",
        "        \n",
        "        #unload the RAM\n",
        "        predictions.clear()\n",
        "        targets.clear()\n",
        "    \n",
        "      X, y = batch\n",
        "      pred = model.predict(X)\n",
        "      batch_predictions = decode_batch_predictions(pred) \n",
        "      predictions.extend(batch_predictions)\n",
        "      for label in y:\n",
        "        label = (tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\"))\n",
        "        targets.append(label)\n",
        "\n",
        "    # length of test dataset is not an exact multiple of 512\n",
        "    if len(targets) > 0:\n",
        "      temp_wer = temp_wer + wer(targets, predictions) * len(targets)\n",
        "      number_of_examples = number_of_examples + len(targets)\n",
        "\n",
        "    final_wer = temp_wer / number_of_examples\n",
        "    print(f\"Number of examples in test dataset: {number_of_examples}\")\n",
        "    print(f\"WER on test: {final_wer}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}